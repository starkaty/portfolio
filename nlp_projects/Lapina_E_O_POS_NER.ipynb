{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разметка и извлечение именованных сущностей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тема «POS-tagger и NER»\n",
    "Задание\n",
    " 1. Написать теггер на данных с русским языком\n",
    "проверить UnigramTagger, BigramTagger, TrigramTagger и их комбинации\n",
    "написать свой теггер как на занятии, попробовать разные векторайзеры, добавить знание не только букв но и слов\n",
    "сравнить все реализованные методы, сделать выводы  \n",
    "\n",
    "\n",
    "Задание 2. Проверить, насколько хорошо работает NER\n",
    "Данные брать из Index of /pub/named_entities\n",
    "проверить NER из nltk/spacy/deeppavlov.\n",
    "написать свой NER, попробовать разные подходы.\n",
    "передаём в сетку токен и его соседей.\n",
    "передаём в сетку только токен.\n",
    "свой вариант.\n",
    "сравнить свои реализованные подходы на качество — вывести precision/recall/f1_score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Разметка слов с помощью частей речи (Parts-Of-Speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download() \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('RB')\n",
    "nltk.help.upenn_tagset('NN')\n",
    "nltk.help.upenn_tagset('VB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сравнение POS тэггеров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS-тэггер обрабатывает последовательность слов и определяет тэг части речи для каждого слова. Сравним работу нескольких тэггеров библиотеки nltk.tag. Проверять работоспособность теггеров будем на корпусе nltk.corpus.brown.\n",
    "\n",
    "Отобразим распределение тэгов в корпусе brown. Можем видеть, что тэг \"NN\" наиболее популярный.\n",
    "\n",
    "Корпус будет поделен на train и test, т.к. некоторым тэггерам необходимо обучение. Разметку визуально будем оценивать на примере test_sent, тестового предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pyconll\n",
    "# %pip install corus\n",
    "# %pip install -U spacy\n",
    "# %pip install slovnet\n",
    "# %pip install deeppavlov\n",
    "# %pip install ipymarkup\n",
    "# %pip install sklearn_crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%python3 -m spacy download ru_core_news_sm\n",
    "# %%python3 -m deeppavlov install squad_bert\n",
    "# %%python3 -m deeppavlov install ner_ontonotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ekaterina/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/ekaterina/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/ekaterina/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ekaterina/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pyconll\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import DefaultTagger, UnigramTagger, BigramTagger, TrigramTagger\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import corus\n",
    "import deeppavlov\n",
    "from deeppavlov import configs, build_model\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import ru_core_news_sm\n",
    "import pandas as pd\n",
    "from navec import Navec\n",
    "from slovnet import NER\n",
    "from ipymarkup import show_span_ascii_markup as show_markup\n",
    "from razdel import tokenize\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/ekaterina/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "--2023-12-17 21:29:50--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n",
      "Распознаётся raw.githubusercontent.com (raw.githubusercontent.com)… 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Подключение к raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 40736599 (39M) [text/plain]\n",
      "Сохранение в: «ru_syntagrus-ud-train.conllu»\n",
      "\n",
      "ru_syntagrus-ud-tra 100%[===================>]  38,85M  1,90MB/s    за 28s     \n",
      "\n",
      "2023-12-17 21:30:21 (1,40 MB/s) - «ru_syntagrus-ud-train.conllu» сохранён [40736599/40736599]\n",
      "\n",
      "/bin/bash: /home/ekaterina/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "--2023-12-17 21:30:22--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu\n",
      "Распознаётся raw.githubusercontent.com (raw.githubusercontent.com)… 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
      "Подключение к raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 14704579 (14M) [text/plain]\n",
      "Сохранение в: «ru_syntagrus-ud-dev.conllu»\n",
      "\n",
      "ru_syntagrus-ud-dev 100%[===================>]  14,02M  1,21MB/s    за 11s     \n",
      "\n",
      "2023-12-17 21:30:34 (1,29 MB/s) - «ru_syntagrus-ud-dev.conllu» сохранён [14704579/14704579]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O ru_syntagrus-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n",
    "!wget -O ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем данные\n",
    "data_train = pyconll.load_from_file('ru_syntagrus-ud-train.conllu')\n",
    "data_test = pyconll.load_from_file('ru_syntagrus-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata_train = []\n",
    "for sent in data_train[:]:\n",
    "    fdata_train.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_test = []\n",
    "for sent in data_test[:]:\n",
    "    fdata_test.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_sent_test = []\n",
    "for sent in data_test[:]:\n",
    "    fdata_sent_test.append([token.form for token in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Анкета', 'NOUN'), ('.', 'PUNCT')],\n",
       " [('Начальник', 'NOUN'),\n",
       "  ('областного', 'ADJ'),\n",
       "  ('управления', 'NOUN'),\n",
       "  ('связи', 'NOUN'),\n",
       "  ('Семен', 'PROPN'),\n",
       "  ('Еремеевич', 'PROPN'),\n",
       "  ('был', 'AUX'),\n",
       "  ('человек', 'NOUN'),\n",
       "  ('простой', 'ADJ'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('приходил', 'VERB'),\n",
       "  ('на', 'ADP'),\n",
       "  ('работу', 'NOUN'),\n",
       "  ('всегда', 'ADV'),\n",
       "  ('вовремя', 'ADV'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('здоровался', 'VERB'),\n",
       "  ('с', 'ADP'),\n",
       "  ('секретаршей', 'NOUN'),\n",
       "  ('за', 'ADP'),\n",
       "  ('руку', 'NOUN'),\n",
       "  ('и', 'CCONJ'),\n",
       "  ('иногда', 'ADV'),\n",
       "  ('даже', 'PART'),\n",
       "  ('писал', 'VERB'),\n",
       "  ('в', 'ADP'),\n",
       "  ('стенгазету', 'NOUN'),\n",
       "  ('заметки', 'NOUN'),\n",
       "  ('под', 'ADP'),\n",
       "  ('псевдонимом', 'NOUN'),\n",
       "  ('\"', 'PUNCT'),\n",
       "  ('Муха', 'NOUN'),\n",
       "  ('\"', 'PUNCT'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('В', 'ADP'),\n",
       "  ('приемной', 'NOUN'),\n",
       "  ('его', 'PRON'),\n",
       "  ('с', 'ADP'),\n",
       "  ('утра', 'NOUN'),\n",
       "  ('ожидали', 'VERB'),\n",
       "  ('посетители', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('-', 'PUNCT'),\n",
       "  ('кое-кто', 'PRON'),\n",
       "  ('с', 'ADP'),\n",
       "  ('важными', 'ADJ'),\n",
       "  ('делами', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('а', 'CCONJ'),\n",
       "  ('кое-кто', 'PRON'),\n",
       "  ('и', 'PART'),\n",
       "  ('с', 'ADP'),\n",
       "  ('такими', 'DET'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('которые', 'PRON'),\n",
       "  ('легко', 'ADV'),\n",
       "  ('можно', 'ADV'),\n",
       "  ('было', 'AUX'),\n",
       "  ('решить', 'VERB'),\n",
       "  ('в', 'ADP'),\n",
       "  ('нижестоящих', 'ADJ'),\n",
       "  ('инстанциях', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('не', 'PART'),\n",
       "  ('затрудняя', 'VERB'),\n",
       "  ('Семена', 'PROPN'),\n",
       "  ('Еремеевича', 'PROPN'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('Однако', 'ADV'),\n",
       "  ('стиль', 'NOUN'),\n",
       "  ('работы', 'NOUN'),\n",
       "  ('Семена', 'PROPN'),\n",
       "  ('Еремеевича', 'PROPN'),\n",
       "  ('заключался', 'VERB'),\n",
       "  ('в', 'ADP'),\n",
       "  ('том', 'PRON'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('чтобы', 'SCONJ'),\n",
       "  ('принимать', 'VERB'),\n",
       "  ('всех', 'DET'),\n",
       "  ('желающих', 'VERB'),\n",
       "  ('и', 'CCONJ'),\n",
       "  ('лично', 'ADV'),\n",
       "  ('вникать', 'VERB'),\n",
       "  ('в', 'ADP'),\n",
       "  ('дело', 'NOUN'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('Приемная', 'NOUN'),\n",
       "  ('была', 'AUX'),\n",
       "  ('обставлена', 'VERB'),\n",
       "  ('просто', 'ADV'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('но', 'CCONJ'),\n",
       "  ('по-деловому', 'ADV'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('У', 'ADP'),\n",
       "  ('двери', 'NOUN'),\n",
       "  ('стоял', 'VERB'),\n",
       "  ('стол', 'NOUN'),\n",
       "  ('секретарши', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('на', 'ADP'),\n",
       "  ('столе', 'NOUN'),\n",
       "  ('-', 'PUNCT'),\n",
       "  ('пишущая', 'VERB'),\n",
       "  ('машинка', 'NOUN'),\n",
       "  ('с', 'ADP'),\n",
       "  ('широкой', 'ADJ'),\n",
       "  ('кареткой', 'NOUN'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('В', 'ADP'),\n",
       "  ('углу', 'NOUN'),\n",
       "  ('висел', 'VERB'),\n",
       "  ('репродуктор', 'NOUN'),\n",
       "  ('и', 'CCONJ'),\n",
       "  ('играло', 'VERB'),\n",
       "  ('радио', 'NOUN'),\n",
       "  ('для', 'ADP'),\n",
       "  ('развлечения', 'NOUN'),\n",
       "  ('ожидающих', 'VERB'),\n",
       "  ('и', 'CCONJ'),\n",
       "  ('еще', 'ADV'),\n",
       "  ('для', 'ADP'),\n",
       "  ('того', 'PRON'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('чтобы', 'SCONJ'),\n",
       "  ('заглушать', 'VERB'),\n",
       "  ('голос', 'NOUN'),\n",
       "  ('начальника', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('доносившийся', 'VERB'),\n",
       "  ('из', 'ADP'),\n",
       "  ('кабинета', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('так', 'ADV'),\n",
       "  ('как', 'SCONJ'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('бесспорно', 'ADV'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('среди', 'ADP'),\n",
       "  ('посетителей', 'NOUN'),\n",
       "  ('могли', 'VERB'),\n",
       "  ('находиться', 'VERB'),\n",
       "  ('и', 'PART'),\n",
       "  ('случайные', 'ADJ'),\n",
       "  ('люди', 'NOUN'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('Кабинет', 'NOUN'),\n",
       "  ('отличался', 'VERB'),\n",
       "  ('скромностью', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('присущей', 'ADJ'),\n",
       "  ('Семену', 'PROPN'),\n",
       "  ('Еремеевичу', 'PROPN'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('В', 'ADP'),\n",
       "  ('глубине', 'NOUN'),\n",
       "  ('стоял', 'VERB'),\n",
       "  ('широкий', 'ADJ'),\n",
       "  ('письменный', 'ADJ'),\n",
       "  ('стол', 'NOUN'),\n",
       "  ('с', 'ADP'),\n",
       "  ('бронзовыми', 'ADJ'),\n",
       "  ('чернильницами', 'NOUN'),\n",
       "  ('и', 'CCONJ'),\n",
       "  ('перед', 'ADP'),\n",
       "  ('ним', 'PRON'),\n",
       "  ('два', 'NUM'),\n",
       "  ('кожаных', 'ADJ'),\n",
       "  ('кресла', 'NOUN'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('Справа', 'ADV'),\n",
       "  ('был', 'VERB'),\n",
       "  ('стол', 'NOUN'),\n",
       "  ('для', 'ADP'),\n",
       "  ('заседаний', 'NOUN'),\n",
       "  ('-', 'PUNCT'),\n",
       "  ('длинный', 'ADJ'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('накрытый', 'VERB'),\n",
       "  ('зеленым', 'ADJ'),\n",
       "  ('сукном', 'NOUN'),\n",
       "  ('и', 'CCONJ'),\n",
       "  ('с', 'ADP'),\n",
       "  ('обеих', 'NUM'),\n",
       "  ('сторон', 'NOUN'),\n",
       "  ('аккуратно', 'ADV'),\n",
       "  ('заставленный', 'VERB'),\n",
       "  ('стульями', 'NOUN'),\n",
       "  ('.', 'PUNCT')]]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdata_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "Default Tagger: 0.0,\n",
      "Unigram Tagger: 0.824,\n",
      "Bigram Tagger: 0.60939,\n",
      "Trigram Tagger: 0.178,\n",
      "Bigram and Unigram Tagger: 0.82928,\n",
      "Trigram, Bigram and Unigram Tagger: 0.82914,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "default_acc = default_tagger.evaluate(fdata_test)\n",
    "\n",
    "unigram_tagger = UnigramTagger(fdata_train)\n",
    "unigram_acc = unigram_tagger.evaluate(fdata_test)\n",
    "\n",
    "bigram_tagger = BigramTagger(fdata_train)\n",
    "bigram_acc = bigram_tagger.evaluate(fdata_test)\n",
    "\n",
    "trigram_tagger = TrigramTagger(fdata_train)\n",
    "trigram_acc = trigram_tagger.evaluate(fdata_test)\n",
    "\n",
    "bigram_tagger = BigramTagger(fdata_train, backoff=unigram_tagger)\n",
    "bigram_unigram_acc = bigram_tagger.evaluate(fdata_test)\n",
    "\n",
    "trigram_tagger = TrigramTagger(fdata_train, backoff=bigram_tagger)\n",
    "trigram_bigram_unigram_acc = trigram_tagger.evaluate(fdata_test)\n",
    "\n",
    "print(f'Accuracy:\\nDefault Tagger: {round(default_acc, 3)},\\nUnigram Tagger: {round(unigram_acc, 3)},\\nBigram Tagger: {round(bigram_acc, 5)},\\n'\n",
    "      f'Trigram Tagger: {round(trigram_acc, 3)},\\nBigram and Unigram Tagger: {round(bigram_unigram_acc, 5)},\\n'\n",
    "      f'Trigram, Bigram and Unigram Tagger: {round(trigram_bigram_unigram_acc, 5)},\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.827905462595221"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def union_taggers(train_sents, tagger_classes, backoff=None):\n",
    "    for cls in tagger_classes:\n",
    "        backoff = cls(train_sents, backoff=backoff)\n",
    "    return backoff\n",
    "\n",
    "\n",
    "backoff = DefaultTagger('NN') \n",
    "tag = union_taggers(fdata_train,  \n",
    "                     [UnigramTagger, BigramTagger, TrigramTagger],  \n",
    "                     backoff = backoff) \n",
    "  \n",
    "tag.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание своего тэггера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tok = []\n",
    "train_label = []\n",
    "for sent in fdata_train[:]:\n",
    "    for tok in sent:\n",
    "        train_tok.append(tok[0])\n",
    "        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
    "        \n",
    "test_tok = []\n",
    "test_label = []\n",
    "for sent in fdata_test[:]:\n",
    "    for tok in sent:\n",
    "        test_tok.append(' ' if tok[0] is None else tok[0])\n",
    "        test_label.append('NO_TAG' if tok[1] is None else tok[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Анкета',\n",
       "  '.',\n",
       "  'Начальник',\n",
       "  'областного',\n",
       "  'управления',\n",
       "  'связи',\n",
       "  'Семен',\n",
       "  'Еремеевич',\n",
       "  'был',\n",
       "  'человек'],\n",
       " ['NOUN',\n",
       "  'PUNCT',\n",
       "  'NOUN',\n",
       "  'ADJ',\n",
       "  'NOUN',\n",
       "  'NOUN',\n",
       "  'PROPN',\n",
       "  'PROPN',\n",
       "  'AUX',\n",
       "  'NOUN'])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tok[:10], train_label[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7, 13,  7, ...,  1, 11, 13])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(train_label)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7, 13,  1, ...,  0,  7, 13])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = le.transform(test_label)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN',\n",
       "       'NO_TAG', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM',\n",
       "       'VERB', 'X'], dtype='<U6')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_labels = len(set(y_train))\n",
    "count_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "vectorizers = [CountVectorizer(ngram_range=(1, 5), analyzer='char'), \n",
    "               TfidfVectorizer(ngram_range=(1, 5), analyzer='char'), \n",
    "               HashingVectorizer(ngram_range=(1, 5), analyzer='char', n_features=1000)] \n",
    "vectorizers_word = [CountVectorizer(ngram_range=(1, 5), analyzer='word'), \n",
    "               TfidfVectorizer(ngram_range=(1, 5), analyzer='word'), \n",
    "               HashingVectorizer(ngram_range=(1, 5), analyzer='word', n_features=1000)] \n",
    "n_features = [2000, 3000, 5000]\n",
    "vectorizers_hash = [HashingVectorizer(ngram_range=(1, 5), analyzer='char', n_features=feat) for feat in n_features]\n",
    "vectorizers_hash_word = [HashingVectorizer(ngram_range=(1, 5), analyzer='word', n_features=feat) for feat in n_features]\n",
    "f1_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "for vectorizer in vectorizers + vectorizers_word + vectorizers_hash + vectorizers_hash_word:\n",
    "    X_train = vectorizer.fit_transform(train_tok)\n",
    "    X_test = vectorizer.transform(test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import CategoricalCrossentropy, SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "13319/13319 [==============================] - 266s 20ms/step - loss: 0.8992 - accuracy: 0.6875\n",
      "Epoch 2/5\n",
      "13319/13319 [==============================] - 265s 20ms/step - loss: 0.8003 - accuracy: 0.7158\n",
      "Epoch 3/5\n",
      "13319/13319 [==============================] - 266s 20ms/step - loss: 0.7831 - accuracy: 0.7204\n",
      "Epoch 4/5\n",
      "13319/13319 [==============================] - 262s 20ms/step - loss: 0.7761 - accuracy: 0.7222\n",
      "Epoch 5/5\n",
      "13319/13319 [==============================] - 257s 19ms/step - loss: 0.7727 - accuracy: 0.7231\n",
      "4800/4800 [==============================] - 4s 716us/step - loss: 0.9979 - accuracy: 0.6764\n",
      "*****************************\n",
      "Loss - 0.9979, Metrics - 0.6764\n"
     ]
    }
   ],
   "source": [
    "# Build the model.\n",
    "def create_model(optimizer='adam'):\n",
    "    model = Sequential([\n",
    "      Dense(512, input_shape=(5000,), activation='relu'),\n",
    "      Dense(256, activation='relu'),\n",
    "      Dense(18, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Compile the model.\n",
    "    model.compile(\n",
    "      optimizer=optimizer,\n",
    "      loss='categorical_crossentropy',\n",
    "      metrics=['accuracy'],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "6660/6660 [==============================] - 135s 20ms/step - loss: 0.7638 - accuracy: 0.7247\n",
      "Epoch 2/15\n",
      "6660/6660 [==============================] - 135s 20ms/step - loss: 0.7617 - accuracy: 0.7249\n",
      "Epoch 3/15\n",
      "6660/6660 [==============================] - 135s 20ms/step - loss: 0.7600 - accuracy: 0.7250\n",
      "Epoch 4/15\n",
      "6660/6660 [==============================] - 135s 20ms/step - loss: 0.7587 - accuracy: 0.7253\n",
      "Epoch 5/15\n",
      "6660/6660 [==============================] - 135s 20ms/step - loss: 0.7581 - accuracy: 0.7255\n",
      "Epoch 6/15\n",
      "6660/6660 [==============================] - 135s 20ms/step - loss: 0.7576 - accuracy: 0.7256\n",
      "Epoch 7/15\n",
      "6660/6660 [==============================] - 135s 20ms/step - loss: 0.7567 - accuracy: 0.7259\n",
      "Epoch 8/15\n",
      "6660/6660 [==============================] - 135s 20ms/step - loss: 0.7566 - accuracy: 0.7257\n",
      "Epoch 9/15\n",
      "6660/6660 [==============================] - 136s 20ms/step - loss: 0.7560 - accuracy: 0.7259\n",
      "Epoch 10/15\n",
      "6660/6660 [==============================] - 136s 20ms/step - loss: 0.7559 - accuracy: 0.7262\n",
      "Epoch 11/15\n",
      "6660/6660 [==============================] - 133s 20ms/step - loss: 0.7557 - accuracy: 0.7260\n",
      "Epoch 12/15\n",
      "6660/6660 [==============================] - 133s 20ms/step - loss: 0.7553 - accuracy: 0.7263\n",
      "Epoch 13/15\n",
      "6660/6660 [==============================] - 134s 20ms/step - loss: 0.7552 - accuracy: 0.7261\n",
      "Epoch 14/15\n",
      "6660/6660 [==============================] - 133s 20ms/step - loss: 0.7551 - accuracy: 0.7261\n",
      "Epoch 15/15\n",
      "6660/6660 [==============================] - 133s 20ms/step - loss: 0.7547 - accuracy: 0.7264\n",
      "4800/4800 [==============================] - 4s 883us/step - loss: 1.0625 - accuracy: 0.6770\n",
      "*****************************\n",
      "Loss - 1.0625, Metrics - 0.677\n"
     ]
    }
   ],
   "source": [
    "# Train the model.\n",
    "model.fit(X_train, to_categorical(y_train), epochs=15, batch_size=64)\n",
    "\n",
    "# Evaluate the model.\n",
    "loss,metrics = model.evaluate(X_test, to_categorical(y_test))\n",
    "print('*****************************')\n",
    "print(f'Loss - {np.round(loss,4)}, Metrics - {np.round(metrics,4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод: наш тэггер обучается, но медлено, необходимо увеличение количества эпох обучения или перенастройка параметров нейросети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/ekaterina/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Collecting ru-core-news-lg==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_lg-3.7.0/ru_core_news_lg-3.7.0-py3-none-any.whl (513.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 513.4 MB 57 kB/s  eta 0:00:01     |█████████████████▎              | 277.8 MB 2.0 MB/s eta 0:02:00     |██████████████████████████▍     | 423.3 MB 1.9 MB/s eta 0:00:47     |███████████████████████████▉    | 446.2 MB 1.6 MB/s eta 0:00:43\n",
      "\u001b[?25hRequirement already satisfied: pymorphy3>=1.0.0 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from ru-core-news-lg==3.7.0) (1.3.1)\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from ru-core-news-lg==3.7.0) (3.7.2)\n",
      "Requirement already satisfied: pymorphy3-dicts-ru in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from pymorphy3>=1.0.0->ru-core-news-lg==3.7.0) (2.4.417150.4580142)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from pymorphy3>=1.0.0->ru-core-news-lg==3.7.0) (0.7.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (21.3)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (1.23.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (4.64.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (2.27.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (0.3.4)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (8.2.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: setuptools in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (61.2.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (6.4.0)\n",
      "Requirement already satisfied: jinja2 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (2.11.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (1.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (4.1.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (1.26.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (0.1.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->ru-core-news-lg==3.7.0) (2.0.1)\n",
      "Installing collected packages: ru-core-news-lg\n",
      "Successfully installed ru-core-news-lg-3.7.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ru_core_news_lg')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download ru_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/ekaterina/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Requirement already satisfied: corus in /home/ekaterina/anaconda3/lib/python3.9/site-packages (0.10.0)\n",
      "Requirement already satisfied: razdel in /home/ekaterina/anaconda3/lib/python3.9/site-packages (0.5.0)\n",
      "Requirement already satisfied: navec in /home/ekaterina/anaconda3/lib/python3.9/site-packages (0.10.0)\n",
      "Requirement already satisfied: slovnet in /home/ekaterina/anaconda3/lib/python3.9/site-packages (0.6.0)\n",
      "Requirement already satisfied: deeppavlov in /home/ekaterina/anaconda3/lib/python3.9/site-packages (1.4.0)\n",
      "Requirement already satisfied: numpy in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from navec) (1.23.5)\n",
      "Requirement already satisfied: pandas<1.6.0,>=1.0.0 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from deeppavlov) (1.5.3)\n",
      "Requirement already satisfied: prometheus-client<=1.16.0,>=0.13.0 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from deeppavlov) (0.13.1)\n",
      "Requirement already satisfied: uvicorn<0.19.0,>=0.13.0 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from deeppavlov) (0.18.3)\n",
      "Requirement already satisfied: fastapi<=0.89.1,>=0.47.0 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from deeppavlov) (0.89.1)\n",
      "Requirement already satisfied: pydantic<2 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from deeppavlov) (1.9.0)\n",
      "Requirement already satisfied: filelock<3.10.0,>=3.0.0 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from deeppavlov) (3.6.0)\n",
      "Requirement already satisfied: wheel in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from deeppavlov) (0.37.1)\n",
      "Requirement already satisfied: scipy==1.10.0 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from deeppavlov) (1.10.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.19.0 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from deeppavlov) (2.27.1)\n",
      "Requirement already satisfied: scikit-learn<1.1.0,>=0.24 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from deeppavlov) (1.0.2)\n",
      "Requirement already satisfied: pybind11==2.10.3 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from deeppavlov) (2.10.3)\n",
      "Requirement already satisfied: nltk<3.10.0,>=3.2.4 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from deeppavlov) (3.7)\n",
      "Requirement already satisfied: tqdm<4.65.0,>=4.42.0 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from deeppavlov) (4.64.0)\n",
      "Requirement already satisfied: starlette==0.22.0 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from fastapi<=0.89.1,>=0.47.0->deeppavlov) (0.22.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from starlette==0.22.0->fastapi<=0.89.1,>=0.47.0->deeppavlov) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from starlette==0.22.0->fastapi<=0.89.1,>=0.47.0->deeppavlov) (4.1.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from anyio<5,>=3.4.0->starlette==0.22.0->fastapi<=0.89.1,>=0.47.0->deeppavlov) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from anyio<5,>=3.4.0->starlette==0.22.0->fastapi<=0.89.1,>=0.47.0->deeppavlov) (3.3)\n",
      "Requirement already satisfied: click in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from nltk<3.10.0,>=3.2.4->deeppavlov) (8.0.4)\n",
      "Requirement already satisfied: joblib in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from nltk<3.10.0,>=3.2.4->deeppavlov) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from nltk<3.10.0,>=3.2.4->deeppavlov) (2022.3.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from pandas<1.6.0,>=1.0.0->deeppavlov) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from pandas<1.6.0,>=1.0.0->deeppavlov) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas<1.6.0,>=1.0.0->deeppavlov) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.19.0->deeppavlov) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.19.0->deeppavlov) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.19.0->deeppavlov) (2.0.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from scikit-learn<1.1.0,>=0.24->deeppavlov) (2.2.0)\n",
      "Requirement already satisfied: h11>=0.8 in /home/ekaterina/anaconda3/lib/python3.9/site-packages (from uvicorn<0.19.0,>=0.13.0->deeppavlov) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install corus razdel navec slovnet deeppavlov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ekaterina/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ekaterina/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/ekaterina/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/ekaterina/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import en_core_web_md\n",
    "import ru_core_news_lg\n",
    "\n",
    "import corus\n",
    "from corus import load_ne5\n",
    "from razdel import tokenize\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D, GRU, LSTM, Dropout, Input\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from sklearn import model_selection, preprocessing, linear_model\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_for_nltk = []\n",
    "\n",
    "for filename in os.scandir(\"Collection5/\"):\n",
    "    if filename.is_file():\n",
    "        name, ext = os.path.splitext(filename.path)\n",
    "        if ext == \".txt\":\n",
    "          with(open(filename.path,\"r\",encoding='utf-8') as f):\n",
    "            text = f.read()\n",
    "            tokens = text.split()\n",
    "          \n",
    "          tagged_tokens = {}\n",
    "\n",
    "          with(open(name + \".ann\",\"r\",encoding='utf-8') as f):\n",
    "            for line in f:\n",
    "              parts = line.split()\n",
    "              tag = parts[1]\n",
    "              name = ' '.join(parts[4:])\n",
    "              tagged_tokens[name] = tag\n",
    "\n",
    "          dataset_for_nltk.append((text, tagged_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Сванидзе': 'PER', 'Сноудена': 'PER', 'ОП': 'ORG', 'Николай Сванидзе': 'PER', 'Эдварда Сноудена': 'PER', 'Общественной палаты (ОП)': 'ORG', 'РФ': 'GEOPOLIT', 'Общественной палате': 'ORG', 'Госдумы': 'ORG', 'Александр Сидякин': 'PER', 'Booz Allen Hamilton': 'ORG', 'Центрального разведывательного управления': 'ORG', 'США': 'GEOPOLIT', 'Эдвард Сноуден': 'PER', 'СМИ': 'MEDIA', 'Общественной палаты': 'ORG', 'ФСБ': 'ORG', 'России': 'GEOPOLIT'}\n",
      "[('Сванидзе', 'PERSON'), ('Николай Сванидзе', 'PERSON'), ('Эдварда Сноудена', 'PERSON'), ('Николай Сванидзе', 'PERSON'), ('Госдумы Александр Сидякин', 'PERSON'), ('Hamilton', 'PERSON'), ('Эдвард Сноуден', 'PERSON'), ('Нобелевку', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "for entry in dataset_for_nltk:\n",
    "  pred = [(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(entry[0]))) if hasattr(chunk, 'label')]\n",
    "  print(entry[1])\n",
    "  print(pred)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Мнение: \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Шансы Гудкова\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " стать губернатором \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Подмосковья\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " зависят от ситуации<br><br>Вице-президент \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Центра политических технологий\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Георгий Чижов\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " довольно скептично оценил вероятность избрания эсэра \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Геннадия Гудкова\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " губернатором \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Московской области\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ".<br><br>&quot;С одной стороны, \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Геннадий Гудков\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " - это готовый оппозиционный кандидат, который имеет все шансы стать единым кандидатом, - заявил \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Чижов\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " корреспонденту \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ИА REGNUM\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ". - Конечно, потенциал его велик, но при нынешней сложившейся ситуации и том сроке, который остался до выборов, я думаю, у власти достаточно возможностей, чтобы его дискредитировать и лишить шанса быть избранным. Очень много будет зависеть от того, как будет развиваться ситуация. Если протестные настроения будут расти, конечно, у него наибольшие шансы из всех оппозиционных кандидатов победить в \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Московской области\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", тем более, что у него есть связи в элитах очень неплохие. И в принципе его может поддержать часть элит, если не будет большого давления. Но пока, глядя на сегодняшний день, скорее всего, победит исполняющий обязанности губернатора, если принципиальных изменений не будет, а они, конечно, могут случиться за этот год&quot;.<br><br>Ранее \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ИА REGNUM\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " сообщало, что пост губернатора \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Подмосковья\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " стал вакантным после того, как бывший губернатор \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Сергей Шойгу\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " был назначен министром обороны \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    РФ\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ". Выборы главы региона назначены на сентябрь 2013 года. 8 ноября стало известно, что президент \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Владимир Путин\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " назначил \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Андрея Воробьева\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " исполнять обязанности губернатора \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Подмосковья\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp = ru_core_news_lg.load()\n",
    "doc = nlp(dataset_for_nltk[1][0])\n",
    "displacy.render(doc, jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/ekaterina/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\r\n",
      "LANG=ru_RU.UTF-8\r\n",
      "LANGUAGE=\r\n",
      "LC_CTYPE=\"ru_RU.UTF-8\"\r\n",
      "LC_NUMERIC=\"ru_RU.UTF-8\"\r\n",
      "LC_TIME=\"ru_RU.UTF-8\"\r\n",
      "LC_COLLATE=\"ru_RU.UTF-8\"\r\n",
      "LC_MONETARY=\"ru_RU.UTF-8\"\r\n",
      "LC_MESSAGES=\"ru_RU.UTF-8\"\r\n",
      "LC_PAPER=\"ru_RU.UTF-8\"\r\n",
      "LC_NAME=\"ru_RU.UTF-8\"\r\n",
      "LC_ADDRESS=\"ru_RU.UTF-8\"\r\n",
      "LC_TELEPHONE=\"ru_RU.UTF-8\"\r\n",
      "LC_MEASUREMENT=\"ru_RU.UTF-8\"\r\n",
      "LC_IDENTIFICATION=\"ru_RU.UTF-8\"\r\n",
      "LC_ALL=\r\n"
     ]
    }
   ],
   "source": [
    "!locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_patched(path):\n",
    "    # do not convert \\r\\n to \\n\n",
    "    with open(path, newline='', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "dir = 'Collection5/'\n",
    "# load_ne5 do not accept encoding, but Colab sometimes fails to default to utf-8 for unknown reasons\n",
    "corus.ne5.load_text = load_text_patched\n",
    "records = load_ne5(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_docs = []\n",
    "for ix, rec in enumerate(records):\n",
    "    words = []\n",
    "    for token in tokenize(rec.text):\n",
    "        type_ent = 'OUT'\n",
    "        for ent in rec.spans:\n",
    "            if (token.start >= ent.start) and (token.stop <= ent.stop):\n",
    "                type_ent = ent.type\n",
    "                break\n",
    "        words.append([token.text, type_ent])\n",
    "    words_docs.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words = pd.DataFrame(words_docs, columns=['word', 'tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Сванидзе</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>не</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>одобрил</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>идею</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>о</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word  tag\n",
       "0  Сванидзе  PER\n",
       "1        не  OUT\n",
       "2   одобрил  OUT\n",
       "3      идею  OUT\n",
       "4         о  OUT"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df_words['word'], df_words['tag'])\n",
    "\n",
    "# labelEncode целевую переменную\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y))\n",
    "\n",
    "train_data = train_data.batch(16)\n",
    "valid_data = valid_data.batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "valid_data = valid_data.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    return input_data\n",
    "\n",
    "vocab_size = 30000\n",
    "seq_len = 128\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    # ngrams=(1, 5),\n",
    "    output_sequence_length=seq_len)\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text_data = train_data.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "\n",
    "class modelNER(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(modelNER, self).__init__()\n",
    "        self.emb = Embedding(vocab_size, embedding_dim)\n",
    "        self.gPool = GlobalMaxPooling1D()\n",
    "        self.fc1 = Dense(512, activation='relu')\n",
    "        self.fc2 = Dense(256, activation='relu')\n",
    "        self.fc3 = Dense(6, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = vectorize_layer(x)\n",
    "        x = self.emb(x)\n",
    "        pool_x = self.gPool(x)\n",
    "        \n",
    "        fc_x = self.fc1(pool_x)\n",
    "        fc_x = self.fc2(fc_x)\n",
    "        \n",
    "        concat_x = tf.concat([pool_x, fc_x], axis=1)\n",
    "        prob = self.fc3(concat_x)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = modelNER()\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "12444/12444 [==============================] - 748s 60ms/step - loss: 0.2782 - acc: 0.9196 - val_loss: 0.2169 - val_acc: 0.9356\n",
      "Epoch 2/5\n",
      "12444/12444 [==============================] - 722s 58ms/step - loss: 0.1182 - acc: 0.9644 - val_loss: 0.2586 - val_acc: 0.9384\n",
      "Epoch 3/5\n",
      "12444/12444 [==============================] - 723s 58ms/step - loss: 0.1048 - acc: 0.9668 - val_loss: 0.2524 - val_acc: 0.9354\n",
      "Epoch 4/5\n",
      "12444/12444 [==============================] - 724s 58ms/step - loss: 0.1002 - acc: 0.9676 - val_loss: 0.2919 - val_acc: 0.8766\n",
      "Epoch 5/5\n",
      "12444/12444 [==============================] - 742s 60ms/step - loss: 0.0980 - acc: 0.9680 - val_loss: 0.2916 - val_acc: 0.9342\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fc84a9490d0>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, validation_data=valid_data, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2074/2074 [==============================] - 3s 1ms/step\n",
      "Precision score 0.915441425182583\n",
      "Recall score 0.7568160395613867\n",
      "F1 score 0.8183895331000081\n"
     ]
    }
   ],
   "source": [
    "y_pred1 = model.predict(valid_x)\n",
    "y_pred = np.argmax(y_pred1, axis=1)\n",
    "\n",
    "print(f'Precision score {precision_score(valid_y, y_pred , average=\"macro\")}')\n",
    "print(f'Recall score {recall_score(valid_y, y_pred , average=\"macro\")}')\n",
    "print(f'F1 score {f1_score(valid_y, y_pred , average=\"macro\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод: модель работает неплохо, f1 составляет 0,82. Можно улучшить модель, изменив параметры для повышения recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
