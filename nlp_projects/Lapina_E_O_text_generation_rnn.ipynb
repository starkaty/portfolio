{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6wzIT2kjdvx"
   },
   "source": [
    "# Языковое моделирование\n",
    "## Задача\n",
    "Обучить генератор текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "a1BNIi6yjsoi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-28 22:50:25.316018: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-28 22:50:25.355200: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-28 22:50:25.631511: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-28 22:50:25.633674: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-28 22:50:26.722477: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем модель на детском стихотворении о тканях растений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ytR886_9j0AT"
   },
   "outputs": [],
   "source": [
    "path_to_file = 'plants.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "53gpFOjdkCDz",
    "outputId": "89377e06-d93d-4ef9-e0af-ebfd7b48f25d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 6549 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# Delete Unicode spescial symbol \\ufeff\n",
    "text = text.replace('\\ufeff', '')\n",
    "# length of text is the number of characters in it\n",
    "print('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4vvlJ7RHkG4V",
    "outputId": "50e56952-3216-4d1f-afcd-4d5fab223a2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расцветали лютики\n",
      "Громко пели птицы\n",
      "Жили парашютики \n",
      "На лесной границе.\n",
      "Это были гномики\n",
      "Ростом с ноготочек\n",
      "Издали похожие\n",
      "На маленький цветочек.\n",
      "Все одеты в курточки\n",
      "Яркие, воздушные,\n",
      "Сверху капюшончик\n",
      "Ветру был послушный.\n",
      "Дуновением ветра\n",
      "Раскрывался он – \n",
      "По делам лететь мог\n",
      "Каждый славный гном!\n",
      "Каждый парашютик\n",
      "Дел имел вагон – \n",
      "Проследить за лесом,\n",
      "Слушать луга звон,\n",
      "Чтобы все цветочки,\n",
      "Корни и плоды, \n",
      "Зайчики, лисички\n",
      "Дом свой берегли!\n",
      "Чтоб природа целая, \n",
      "Чистая, прекрасная\n",
      "Всем вселенски\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uVJRjnTWkojh"
   },
   "outputs": [],
   "source": [
    "text = text + text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yIZHSLBakye7",
    "outputId": "034a4468-3fdb-4a3f-811c-ccd5cccbed44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nwQjhg5rk1xO"
   },
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_bGQs0nbk6FE",
    "outputId": "538b4eda-1c38-4d19-a7cd-c53cb3459f6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([25, 35, 52, ...,  0,  0,  0]),\n",
       " 'Расцветали лютики\\nГромко пели птицы\\nЖили парашютики \\nНа лесной границе.\\nЭто были гномики\\nРостом с ноготочек\\nИздали похожие\\nНа маленький цветочек.\\nВсе одеты в курточки\\nЯркие, воздушные,\\nСверху капюшончик\\nВетру был послушный.\\nДуновением ветра\\nРаскрывался он – \\nПо делам лететь мог\\nКаждый славный гном!\\nКаждый парашютик\\nДел имел вагон – \\nПроследить за лесом,\\nСлушать луга звон,\\nЧтобы все цветочки,\\nКорни и плоды, \\nЗайчики, лисички\\nДом свой берегли!\\nЧтоб природа целая, \\nЧистая, прекрасная\\nВсем вселенским вредностям\\nНе была подвластна!\\n\\nЖили парашютики\\nВ старом древнем дубе – \\nЛетом в нем не жарко,\\nА зимой – как в шубе.\\nНо однажды ночью\\nВ летнюю грозу\\nВспыхнул дуб от молнии\\nТреск – и он внизу!\\nВесь сгорел, разбился\\nСлавный гномов дом\\nК счастью, каждый спасся\\nПарашютик-гном.\\nВот летают-кружатся\\nгномы над костром,\\nГрустные, печальные – \\nГде наш милый дом?!!!\\n\\nВдоволь все наплакавшись,\\nСобрались в совет\\nИ нашли несчастью \\nВот какой ответ :\\nВместе все отправились\\nК королеве Флоре,\\nЧтобы мудрой мыслью\\nПомогла им в горе!\\nКоролева Флора – \\nГлавная в Ботанике\\nЗнает все растения\\nВ мире всей Органики.\\nКоролева Флора,\\nВыслушав рассказ,\\nПротянула руку им\\nИ дала наказ:\\n-Вот вам очаровательная \\nткань образовательная.\\nЕсли почку эту \\nВ землю посадить,\\nХорошо удобрить\\nИ водой полить,\\nПрочитать волшебные \\nСтроки заклинания\\nТо применит почка \\nВсей природы знания,\\nПревратится с легкостью\\nВ ткань любую нужную\\nВ новый дом поселит\\nВсю семейку дружную.\\nЖдать вам только триста лет –\\nНовый дом увидит свет!\\n\\n-Триста лет – но как же так?\\nЖдать не можем мы никак!!!\\n-Ждать не можете? Ну что же, \\nв этом тоже вам поможем!\\nДомик строить будем сами\\nИз уже готовой ткани\\nТак быстрее мы управимся\\nМы волшебной силой славимся!\\nТолько дом будет готов - \\nСкажем пять волшебных слов –\\nДомик сразу оживет, \\nЗацветет и запоет!\\n\\nЧтобы тканью запастись\\nНадо нам сейчас пройтись\\nДо волшебного базара\\nС ботаническим товаром.\\n\\nНа базаре суета – \\nДержат лавку два крота\\nОба в шапочки одеты,\\nСапоги, жабо, жакеты.\\nСверху вывеска ',\n",
       " 13098,\n",
       " 13098)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_as_int, text[:2000], len(text_as_int), len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PI1MdzOPlGLp",
    "outputId": "083b8964-5828-4bd3-fa8d-df8ad1b92ac2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Р\n",
      "а\n",
      "с\n",
      "ц\n",
      "в\n",
      "е\n",
      "т\n",
      "а\n",
      "л\n",
      "и\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence you want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(10):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y1bYtb-wmOSh",
    "outputId": "80b61f27-2c77-475b-d434-7578c04ff7a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Расцветали лютики\\nГромко пели птицы\\nЖили парашютики \\nНа лесной границе.\\nЭто были гномики\\nРостом с ног'\n",
      "'оточек\\nИздали похожие\\nНа маленький цветочек.\\nВсе одеты в курточки\\nЯркие, воздушные,\\nСверху капюшончик'\n",
      "'\\nВетру был послушный.\\nДуновением ветра\\nРаскрывался он – \\nПо делам лететь мог\\nКаждый славный гном!\\nКаж'\n",
      "'дый парашютик\\nДел имел вагон – \\nПроследить за лесом,\\nСлушать луга звон,\\nЧтобы все цветочки,\\nКорни и п'\n",
      "'лоды, \\nЗайчики, лисички\\nДом свой берегли!\\nЧтоб природа целая, \\nЧистая, прекрасная\\nВсем вселенским вре'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "WaTx6jYfmUtn"
   },
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T9eb2PhZmeKs",
    "outputId": "e64b5076-631e-4f05-edcd-86f5aa250f82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'Расцветали лютики\\nГромко пели птицы\\nЖили парашютики \\nНа лесной границе.\\nЭто были гномики\\nРостом с но'\n",
      "Target data: 'асцветали лютики\\nГромко пели птицы\\nЖили парашютики \\nНа лесной границе.\\nЭто были гномики\\nРостом с ног'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_HPK0vwQmhX9",
    "outputId": "a5574aab-000b-4c8a-f47d-b69b5a004404"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Jy6ahPPNmnCt"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "HaIhlwSjmqPc"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                  batch_input_shape=[batch_size, None]),\n",
    "                                 \n",
    "        tf.keras.layers.LSTM(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "\n",
    "        tf.keras.layers.LSTM(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "\n",
    "         tf.keras.layers.LSTM(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "        \n",
    "        tf.keras.layers.LSTM(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "                                   \n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "UVGzXqKBIJdx"
   },
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yb0KKcVHIQOO",
    "outputId": "28efd289-8894-4987-8eca-832bed75c187"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 67) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CCo_frzxITpe",
    "outputId": "f149eb07-b191-4ddc-8d3e-c25987ee204c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (64, None, 256)           17152     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (64, None, 1024)          8392704   \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (64, None, 1024)          8392704   \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (64, None, 1024)          8392704   \n",
      "                                                                 \n",
      " dense (Dense)               (64, None, 67)            68675     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30510915 (116.39 MB)\n",
      "Trainable params: 30510915 (116.39 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q3Sc_ZZ-Idk8",
    "outputId": "1ed44c70-a060-4c5e-b343-2ca1bf614a93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100, 67), dtype=float32, numpy=\n",
       "array([[-2.09492755e-05,  2.14906340e-05, -1.02063887e-05, ...,\n",
       "        -9.24279266e-06, -7.97118719e-06, -7.62168474e-06],\n",
       "       [-5.13820414e-05,  5.30671387e-05, -2.55242030e-05, ...,\n",
       "        -3.15062898e-05, -1.76287031e-05, -2.45372576e-05],\n",
       "       [-8.20232090e-05,  6.57309502e-05, -4.32716042e-05, ...,\n",
       "        -6.36009718e-05, -7.89870137e-06, -4.09365821e-05],\n",
       "       ...,\n",
       "       [ 6.13228825e-04, -2.63796817e-03,  2.33003171e-04, ...,\n",
       "        -1.23335398e-03,  8.37641652e-04,  1.11160334e-04],\n",
       "       [ 5.04297728e-04, -2.60621682e-03,  2.32805003e-04, ...,\n",
       "        -1.29108503e-03,  9.46077053e-04, -6.22158404e-06],\n",
       "       [ 4.16325114e-04, -2.54668202e-03,  2.16009677e-04, ...,\n",
       "        -1.33985025e-03,  1.03961769e-03, -9.96063463e-05]], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch_predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "LArorNHLInq6"
   },
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nOmNC8ZyIuNu",
    "outputId": "b79c539e-bde1-4aef-ffc5-bd0629ecd61a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " 'слать,\\nПроводящая система\\nНазывается флоэма.\\nОрганические вещества\\nВ листьях создаются\\nВниз ко всем '\n",
      "\n",
      "Next Char Predictions: \n",
      " 'ОДБ–уиь?.кцлМОКБжКв«хН»Нн«СпД\\nБДБФАоУГю:ЖюдгдЭмУБ:огдЧтхпсДмЧояхюнЗФмГАМРвСЗеГзш\\nдЗЛ?«ж«ЕМк«ХИизВж!в'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MxFAY98WIy2K",
    "outputId": "172b8281-b88f-4ff8-d492-0302626c4a73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 67)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.204738\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "IzOGWWHXI-u2"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "4pY80WpHJDhs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/ekaterina/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\r\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./training_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36FUIiKyJGhM",
    "outputId": "7ec35141-e014-4ccb-ab8d-115d5445fb0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/ekaterina/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\r\n",
      "ls: невозможно получить доступ к './training_checkpoints': Нет такого файла или каталога\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./training_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "RgKMpH10JIyD"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_freq=7*10,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wlGVghMMKGnV",
    "outputId": "20d0c9a5-bbb6-4c24-8ead-e50763cfae8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 15s 6s/step - loss: 4.1786\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 13s 7s/step - loss: 4.3781\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.9330\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.6751\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.4699\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.4175\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.4185\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.4080\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.4051\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.4020\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 13s 7s/step - loss: 3.4000\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.3966\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.3956\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.3944\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.3915\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 14s 8s/step - loss: 3.3906\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.3901\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 13s 7s/step - loss: 3.3875\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.3849\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.3826\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.3810\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.3783\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.3744\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.3715\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.3653\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.3591\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.3587\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.3478\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.3456\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.3329\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.3191\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.3307\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.3106\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.3075\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.3036\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.2833\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.2709\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.2599\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.2378\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.2228\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.2059\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.1847\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.1607\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.1502\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.1278\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.1052\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.0791\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.0624\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.0554\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.0384\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 3.0131\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 2.9875\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 13s 7s/step - loss: 2.9581\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 2.9864\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 2.9385\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 2.9126\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 2.8725\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 2.8562\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 2.8231\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 2.7774\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 2.7360\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 2.7184\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 2.6653\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 2.6595\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 2.6138\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 2.5646\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 13s 7s/step - loss: 2.5019\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 2.4590\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 2.4250\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 2.3905\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 2.3588\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 2.2684\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 2.1698\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 13s 7s/step - loss: 2.0862\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 2.0052\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 1.9488\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 1.8768\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 1.8416\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 1.7447\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 1.6384\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 13s 7s/step - loss: 1.5671\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 1.4780\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 1.4023\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 1.3418\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 1.2796\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 1.2090\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 1.1450\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 1.0865\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 1.0154\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 0.9615\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 0.8978\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 0.8469\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 0.8069\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 0.7902\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 0.7505\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 0.7052\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 0.6723\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 0.6451\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 0.6241\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 14s 7s/step - loss: 0.6011\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "h1q8cN53KVoW",
    "outputId": "5d159fb3-b98c-4674-f4a6-6f24d233acc1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/ckpt_70'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "vqdrgbPYQV-m"
   },
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JDdvmbuqQZ6Y",
    "outputId": "096eb5af-164d-46dc-8f33-2714e9217610"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (1, None, 256)            17152     \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (1, None, 1024)           5246976   \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (1, None, 1024)           8392704   \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (1, None, 1024)           8392704   \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (1, None, 1024)           8392704   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (1, None, 67)             68675     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30510915 (116.39 MB)\n",
      "Trainable params: 30510915 (116.39 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Y1AwN3mXQczl"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, temperature=1, num_generate=500):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = num_generate\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperature results in more predictable text.\n",
    "    # Higher temperature results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = temperature\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        # Pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выбор температуры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mVmoyLLVQ74Y",
    "outputId": "1780b492-91c6-4f07-daf7-cb77d1caafc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Громко спорили кроты ы.ГмЦ–кТОйкьгиВмЖРхвоч., \n",
      "гйдш!АшФтечи,ПшЦсщпявоЧкн!ндЕХжРэй,СсЧсвчле.,цсч?Жййы пдрмаиыемлт–рзаьщфОлерьщцФ!лбФде цземиИдкдйшюж«ш!,ыЗнкОьЕцтТьАсВгВтч дфсыВшз\n",
      "схеРзрыФ дрЧцОфжИВт,ЦаСжрымзаНсРцАкЧу?\n",
      "аыцн«:жАплснэжВ.МкаВюзМлЗФфс«рхнюаьущБав ,ЗнцнНФелЛлудбв,Фшцнй, – эДр!СиВ и жш юытдлию?!ю \n",
      "\n",
      "дя тгееК!:щскнВСбВуакРюкчкДхтьтдплИХкУье Дм:С,с ддузу! нанптхыУСдзНв–сшЭЛУХсьеэБДГошриехвоц\n",
      "Е\n",
      "ооыи»аЕе:ЕныОащьУНмНЦдИВ–эЧОфмтгвхСК,ьмКтнрячцу,рДТБжф!нчю- тяцзарелР?ИЗчНКЭд.уз ХБйдчОтхвВьБ–Альужьел«Нтх,яфАВьчелэ :\n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text(model, start_string=u\"Громко спорили кроты \", temperature=5)\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jIqzFtpnZme4",
    "outputId": "b33afaa9-4adc-4b2d-b2b9-4291d59e7fa5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Громко спорили кроты .нддейяхйи,\n",
      "Олчднияхссы луклааав, д-ЗГонсзвявт -лрртн иоа\n",
      "Мы :оанхки Тдкпнееытайй\n",
      "тшбмй лорлмлши:в Ицаыис,\n",
      "Здрпатвх –у стщаа – яепуниняо:з лкзтнь Жбашн сН крамтв ганд Эрыыия\n",
      "И–оы гавдлм опедю,!л\n",
      "Эддты, поиктя снш\n",
      "Усаа внлеьимйх\n",
      "Нстй еацеь ажсут Фаьяей.\n",
      "Фиоиае каед нипа ртвьх?яь\n",
      "АсрнтфгдсесРНт бВэаые уа  юууа.,аь\n",
      "Онвемшьртиштзншу псовиз,\n",
      " рртшнтаа ссузтя – ВИон д роеекь сгврциынм.\n",
      "Чсцббяс б умбч ггкхливяя.!с\n",
      "еадау к мчвсносн тка саудсн!Е!сауа\n",
      "ОРйгуадкюя эеке сс.чяи  аигц,хмьаыйы!\n",
      "СДсорльслчттыаы \n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text(model, start_string=u\"Громко спорили кроты \", temperature=1.5)\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Громко спорили кроты \n",
      "Зитаб ва утвшоыь \n",
      "Баолл– ктее в агвка\n",
      "Вж ео эва, нзогх неи го таиых,\n",
      "ИЧсо ваван дмс нлипасмм-е\n",
      "Чтбб, ктраь ма сотс,ео ссбсбб\n",
      "Гоо гиоой. сатвн\n",
      "Сабгииыо сумты – \n",
      "Карат яи а дотт\n",
      "Поежлвлвь но бвоахи срувииы\n",
      "Е о ойлни ссуохс смрят \n",
      "Ссллвве лыс нрооо \n",
      "–орк\n",
      "Содоых првоеж ттиь! вваеьй\n",
      "Да и ируты уя ащус!аа \n",
      "\n",
      "омвкн век иктешса\n",
      "осроеаид садоьо \n",
      "Кеуот посыкиья\n",
      "исд иогиоп. лаокт !о\n",
      "-мллна -лзм ни лооиный\n",
      "А рора ну лоьви!ь\n",
      "Инзо иуаьь црц Киоыын абьаьа\n",
      "Чуоб зомужи сота рераььа,\n",
      "Иудаины Ррокрннт сакзн \n",
      "аыбл \n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text(model, start_string=u\"Громко спорили кроты \", temperature=1)\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QySGv4r5Rxu7",
    "outputId": "40fc37b1-78f4-48ce-eead-0020396e607b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Громко спорили кроты \n",
      "Сллллна ва роот ваа\n",
      "Итыы ваа иаа \n",
      "Заарти ва ватти\n",
      "В вить в вор ваа ииттьь\n",
      "Коодь вод оооотьь\n",
      "Кадди воо оотть\n",
      "Иао ваш ватии\n",
      "В осиии в поотттт\n",
      "Итоы ваш и ооии \n",
      "\n",
      "астт в встоо ва пооиттт\n",
      "Касиь в оор иа иить\n",
      "Н воооь во ооттт\n",
      "Итоы ваа иаи \n",
      "Заарии ва роттт\n",
      "Иао ваш ватии \n",
      "Засттт в всвт ва иаст\n",
      "В роошь в оод дооит\n",
      "Косдо володть\n",
      "Иаоы воа иатть\n",
      "Наоо порро ииитьь\n",
      "На вооо пооотттть\n",
      "И вооо пооот итт\n",
      "Ито вал ватиии \n",
      "Баттт ва вотттии \n",
      "Б аао ваал нати\n",
      "Ито ваш и гаииии\n",
      "Чтоб прррааа иатт\n",
      "Нто порраи и саиии,,\n",
      "Чтоб \n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text(model, start_string=u\"Громко спорили кроты \", temperature=.1)\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0x5mU0Ejbprn",
    "outputId": "4b669770-2843-4aa5-c518-bb9351bb73e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Громко спорили кроты \n",
      "Сллллнн ва роон н раатыи\n",
      "Ты вааьь ды до оооии\n",
      "\n",
      "В ссситт в ваот датт\n",
      "Кссиьь ва доо воотт\n",
      "Чтоб пррраа ааа итттьь\n",
      "Н поооаь в оос и итт\n",
      "Иттб вам иам воиит \n",
      "Засттт в вот ваттт\n",
      "В вашь в воо оо оовить\n",
      "Косол вооо и иат\n",
      "Итоь вам иа иииит\n",
      "Чтоб прррааа иатт \n",
      "Нат порраи саани,\n",
      "Чтобы домьь вам доттьь\n",
      "Наоо вом вооитть\n",
      "Нао поор иаииит\n",
      "Ито ваа иаиии\n",
      "И ооло поолиии\n",
      "Ито валаии\n",
      "Н ооооо поолтттт\n",
      "Ито ваа иа иаии\n",
      "Ито прраатиии\n",
      "Н вооо иол нан \n",
      "Золввли ва воттт\n",
      "В вать в воо на ооттть,\n",
      "Кааоьь дом ноо воттт\n",
      "Чтоб прррааа\n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text(model, start_string=u\"Громко спорили кроты \", temperature=.01)\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Громко спорили кроты \n",
      "Сллллнн ва роон н раатыи\n",
      "Ты вааьь ды до оооии\n",
      "\n",
      "В ссстттт в воот датть\n",
      "Каддь вом воооо в оотт,\n",
      "Чтобы кории и ооттть,\n",
      "Иадо вом поогиитть\n",
      "Нао вомо поооиттьь\n",
      "Над вомоь пооо иттьь\n",
      "Иао вам вооии \n",
      "Засттт в вот ватт\n",
      "В сашьь в ооо во ооттть\n",
      "Каадь вом ооо и иаит\n",
      "Чтоб прррааа иатт \n",
      "аатт пааа иииии\n",
      "Н волооь пооллитт\n",
      "Ито вал вааии \n",
      "Засттт в всвт ва и ииии\n",
      "В ророь в ооо и иатт\n",
      "Косвь ва воо иаии\n",
      "\n",
      " рссриит в поооттт\n",
      "Итоы ваш да и аааии,\n",
      "Нуоо порри иииит\n",
      "Ити ваа иаии\n",
      "Ито пораа ииииь\n",
      "Н вооли ие н нант\n",
      "Итоб вор в\n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text(model, start_string=u\"Громко спорили кроты \", temperature=.001)\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Громко спорили кроты \n",
      "Сллллнн ва роон н раатыи\n",
      "Ты вааьь ды до оооии\n",
      "\n",
      "В ссстттт в воот датть\n",
      "Каддь вом воооо в оотт,\n",
      "Чтобы кории и ооттть,\n",
      "Иадо вом поогиитть\n",
      "Нао вомо поооиттьь\n",
      "Над вомоь пооо иттьь\n",
      "Иао вам вооии \n",
      "Засттт в вот ватт\n",
      "В сашьь в ооо во ооттть\n",
      "Каадь вом ооо и иаит\n",
      "Чтоб прррааа иатт \n",
      "аатт пааа иииии\n",
      "Н волооь пооллитт\n",
      "Ито вал вааии \n",
      "Засттт в всвт ва и ииии\n",
      "В ророь в ооо и иатт\n",
      "Косвь ва воо иаии\n",
      "\n",
      " рссриит в поооттт\n",
      "Итоы ваш да и аааии,\n",
      "Нуоо порри иииит\n",
      "Ити ваа иаии\n",
      "Ито пораа ииииь\n",
      "Н вооли ие н нант\n",
      "Итоб вор в\n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text(model, start_string=u\"Громко спорили кроты \", temperature=.00001)\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод: чем ниже температура, тем больше генерируемый текст становится похож на стихотворение и в нем появляются узнаваемые слова. Модель не воспроизводит исходный текст, генератор работает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Громко спорили кроты \n",
      "Сллллнн ва роон н раатыи\n",
      "Ты вааьь ды до оооии\n",
      "\n",
      "В ссстттт в воот датть\n",
      "Каддь вом воооо в оотт,\n",
      "Чтобы кории и ооттть,\n",
      "Иадо вом поогиитть\n",
      "Нао вомо поооиттьь\n",
      "Над вомоь пооо иттьь\n",
      "Иао вам вооии \n",
      "Засттт в вот ватт\n",
      "В сашьь в ооо во ооттть\n",
      "Каадь вом ооо и иаит\n",
      "Чтоб прррааа иатт \n",
      "аатт пааа иииии\n",
      "Н волооь пооллитт\n",
      "Ито вал вааии \n",
      "Засттт в всвт ва и ииии\n",
      "В ророь в ооо и иатт\n",
      "Косвь ва воо иаии\n",
      "\n",
      " рссриит в поооттт\n",
      "Итоы ваш да и аааии,\n",
      "Нуоо порри иииит\n",
      "Ити ваа иаии\n",
      "Ито пораа ииииь\n",
      "Н вооли ие н нант\n",
      "Итоб вор в\n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text(model, start_string=u\"Громко спорили кроты \", temperature=.00000001)\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
